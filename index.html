<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="CellCLIP – Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <meta name="keywords" content="Contrastive learning, CLIP, cell morphology, Cell Painting, cross-modal retrieval, intra-modal retrieval, zero-shot Learning, machine learning, computer vision, AI">
  <meta name="author" content="Mingyu Lu, Ethan Weinberger, Chanwoo Kim, Su-In Lee">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="UW AIMS">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="CellCLIP – Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells’ morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g. small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://q8888620002.github.io/CellCLIP-website/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="article:published_time" content="2025-09-21T00:00:00.000Z">
  <meta property="article:author" content="Mingyu Lu">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Cell Painting">
  <meta property="article:tag" content="Contrastive Learning">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="CellCLIP – Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells’ morphological responses to perturbations at an unprecedented scale. The collection of such data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of representing different classes of perturbations (e.g. small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image:alt" content="CellCLIP – Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="CellCLIP – Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning">
  <meta name="citation_author" content="Lu, Mingyu">
  <meta name="citation_author" content="Weinberger, Ethan">
  <meta name="citation_author" content="Chanwoo, Kim">
  <meta name="citation_author" content="Su-In, Lee">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Conference on Neural Information Processing Systems">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>CellCLIP - Mingyu Lu, Ethan Weinberger, Chanwoo Kim, Su-In Lee | University of Washington </title>
  
  <!-- Favicon and App Icons -->
<!--   <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico"> -->
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            
            <h1 class="title is-1 publication-title">CellCLIP – Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://mingyu-lu.github.io/" target="_blank">Mingyu Lu</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://ethanweinberger.com/" target="_blank">Ethan Weinberger</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://chanwoo.kim/" target="_blank">Chanwoo Kim</a>,
                  </span>
                    <span class="author-block">
                    <a href="https://aims.cs.washington.edu/su-in-lee" target="_blank">Su-In Lee</a>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Paul G. Allen School of Computer Science & Engineering<br>University of Washington<br>NeurIPS 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                  <span class="link-block">
                    <a href="https://github.com/suinleelab/CellCLIP" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.06290" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                  <a href="https://huggingface.co/suinleelab/CellCLIP" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" 
                         alt="Hugging Face" 
                         style="width:1em; height:1em;">
                  </span>
                  <span>CellCLIP checkpoint</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            High-content screening (HCS) assays based on high-throughput microscopy techniques such as Cell Painting have enabled the interrogation of cells’ morphological responses to perturbations at an unprecedented scale. The collection of such
            data promises to facilitate a better understanding of the relationships between different perturbations and their effects on cellular state. Towards achieving this goal, recent advances in cross-modal contrastive learning could, in theory, be
            leveraged to learn a unified latent space that aligns perturbations with their corresponding morphological effects. However, the application of such methods to HCS data is not straightforward due to substantial differences in the semantics of Cell Painting images compared to natural images, and the difficulty of
            representing different classes of perturbations (e.g. small molecule vs CRISPR gene knockout) in a single latent space. In response to these challenges, here we introduce CellCLIP, a cross-modal contrastive learning framework for HCS data. CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to better capture relationships between different microscopy
            channels in image embeddings, along with natural language encoders for representing perturbations. Our framework outperforms current open-source models, demonstrating the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/website_fig3.jpg" alt="First research result visualization" style="width: 60%; height: auto; display: block; margin: auto;"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Cells’ responses to a perturbation are measured by the Cell Painting assay, which captures a set of distinct cellular components in five imaging channels. 
        </h2>
      </div>
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/example_data1.jpg" alt="First research result visualization" style="width: 50%; height: auto; display: block; margin: auto;"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Example Cell Painting data and text description 
        </h2>
      </div>
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/example_data1.jpg" alt="First research result visualization" style="width: 50%; height: auto; display: block; margin: auto;"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Example Cell Painting data and text description 
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <strong>CellCLIP</strong>, a framework for contrastive learning (CL) on Cell Painting data. 
            Unlike natural image CL methods, our approach accounts for the unique structure of multi-channel cellular images 
            and the many-to-one relationship between perturbations and image sets. CellCLIP integrates three key components:
          </p>

          <h4>CrossChannelFormer</h4>
          <p>
            To encode images, we adapt pretrained natural image models (e.g., DINOv2) by treating each Cell Painting channel as a 
            grayscale input. We then aggregate images from the same perturbation using gated attention pooling, producing a 
            per-perturbation profile. Finally, our <strong>CrossChannelFormer</strong> introduces lightweight cross-channel reasoning 
            by combining pooled profiles with channel-specific embeddings and a global CLS token. This design enables efficient 
            modeling of stain-specific structures while requiring only <em>C+1</em> tokens per perturbation.
          </p>
          
        <div class="content has-text-centered" style="margin-top: 2rem;">
          <img src="static/images/website_fig2.jpg" style="width: 100%; height: auto;" />
          <p class="is-size-10" style="margin-top: 1rem;">
            <em>
              For a perturbation \( i \) , CrossChannelFormer takes as input images of all cells receiving perturbation \( i \)  and pools them into a single embedding \( p_i \)  that takes into account the relationship between information contained in the different Cell Painting channels.
            </em>
          </p>
        </div>
    
          <h4>Perturbation Encoding</h4>
          <p>
            Instead of building modality-specific encoders, we represent perturbations as <strong>natural language prompts</strong>. 
            These descriptions capture both cell type and perturbation details (e.g., compounds or CRISPR targets), enabling 
            a unified representation across diverse perturbation types. We encode prompts with a pretrained BERT model. For example
            to encode the chemical compound butyric acid, a drug affecting cell growth, we use the prompt:
          </p>
          <div class="has-text-centered" style="font-family: monospace; font-style: italic;">
            A cell painting image of U2OS cells treated with butyric acid, SMILES: CCCC(O)=O.
          </div>
          
          <p>Similarly, for a CRISPR perturbation, the prompt is structured as:</p>
          
          <div class="has-text-centered" style="font-family: monospace; font-style: italic;">
            A cell painting image of U2OS cells treated with CRISPR, targeting genes: AP2S1.
          </div>
          <h4>Training Objective</h4>
          <p>
            To align image profiles with perturbation text, we combine the standard CLIP loss with a 
            <strong>Continuously Weighted Contrastive Loss (CWCL)</strong>. CWCL softly reweights pairs based on morphological similarity, 
            ensuring that biologically related perturbations remain close in the embedding space while preserving retrieval performance. 
            This objective balances cross-modal alignment with biologically meaningful intra-modal structure.
          </p>
          For the profile-to-perturbation direction, \( \mathcal{U} \rightarrow \mathcal{V} \), our adapted CWCL objective is given by
          <p style="text-align: center; margin: 1.5rem 0;">
            \[ 
              \mathcal{L}_{\mathrm{CWCL}, \mathcal{U} \rightarrow \mathcal{V}} =
              \frac{1}{N} \sum_{i=1}^N 
              \frac{1}{\sum_{j \in [N]} w_{ij}^{\mathcal{U}}} 
              \left[ 
                \sum_{j=1}^N w_{ij}^{\mathcal{U}} \cdot 
                \log \frac{\exp(\langle p_i \cdot q_j \rangle / \tau )}
                {\sum_{k=1}^N \exp(\langle p_i \cdot q_k \rangle / \tau )}
              \right].
            \]
          </p>
          For the perturbation-to-profile direction, \( \mathcal{V} \rightarrow \mathcal{U} \), we apply the standard CLIP loss. Thus, the final training loss for CellCLIP is 
          <p style="text-align: center; margin: 1.5rem 0;">
            \[
            \mathcal{L}_{\mathrm{total}} = 
            \mathcal{L}_{\mathrm{CWCL}, \mathcal{U} \rightarrow \mathcal{V}} + 
            \mathcal{L}_{\mathrm{CLIP}, \mathcal{V} \rightarrow \mathcal{U}}.
            \]
          </p>
          <p>
            Together, these design choices allow CellCLIP to leverage powerful pretrained models, efficiently represent 
            high-dimensional Cell Painting data, and generalize across diverse perturbation types.
          </p>
          <div class="content has-text-centered" style="margin-top: 2rem;">
            <img src="static/images/website_fig1.jpg" style="width: 100%; height: auto;" />
            <p class="is-size-10" style="margin-top: 1rem;">
              <em>
              CellCLIP aligns perturbation embeddings generated from natural language descriptions with embeddings of corresponding images produced by a new CrossChannelFormer architecture designed to account for the idiosyncracies of Cell Painting.
              </em>
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


  <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
         <div class="content has-text-centered" style="margin-top: 2rem;">
          <img src="static/images/result0.jpg" style="width: 100%; height: auto;" />
          <p class="is-size-10" style="margin-top: 1rem;">
            <em>
            Benchmarking CellCLIP and baseline methods on perturbation-to-profile and profile-to-perturbation retrieval performance for unseen molecules from Bray et al 2016. We report mean Recall@1, @5, and @10 \( \pm \) standard deviation across random seeds for both tasks. Higher recall corresponds to better performance. Best results are shown in <strong>bold</strong>.
            </em>
          </p>
        </div>
          <p>
          <h4>Cross-modality retrieval</h4>
          We present our results for cross-modality retrieval tasks (i.e., perturbation-to-profile and profile-to-perturbation) in Table 1. For our benchmarking we compared CellCLIP against previous state-of-the-art CL methods for Cell Painting perturbation screens: CLOOME and MolPhenix. 
          Overall, we found that CellCLIP demonstrated substantially higher performance at both cross-modal retrieval tasks compared to baseline methods. 
            To understand the source of performance gains in CellCLIP, we conducted a series of ablations to understand the contribution of each component in CellCLIP to retrieval performance. 
            Specifically, starting with CLOOME's proposed encoding scheme, where individual images encoded using ResNet50 are aligned with chemical perturbations encoded using Morgan fingerprints combined with an MLP, we gradually replaced each of CLOOME's components with those of CellCLIP and assessed each change's impact on model performance.
          </p>
          <div class="content has-text-centered" style="margin-top: 2rem;">
            <img src="static/images/result3.jpg" style="width: 100%; height: auto;" />
            <p class="is-size-10" style="margin-top: 1rem;">
              <em>
            Ablation studies of various vision and perturbation encoder combinations and retrieval performance in Bray et al 2016. We report mean Recall@1, @5, and @10 \( \pm \) standard deviation across random seeds for perturbation-to-profile and profile-to-perturbation retrieval tasks.  \(\triangle\) indicates Morgan Fingerprint; \(\Box\) indicates text prompt.
              </em>
            </p>
          </div>
        <h4>
        Language can effectively represent perturbations.
        </h4>
         <p>
        We began by replacing CLOOME's chemical structure encoder, which feeds chemicals' Morgan fingerprints through a multi-layer perceptrion (MLP), with the natural language encoder used in CellCLIP while holding all other model components fixed. We found that this change alone yielded significant performance gains for both retrieval tasks. Notably, we found this result continued to hold even after replacing the generic MLP used in CLOOME with an MPNN++ network designed specifically for molecular property prediction
         </p>

        <h4>
        Cross-channel reasoning improves retrieval performance
        </h4>
          <p>
          We next investigated the impact of CrossChannelFormer's ability to reason across global Cell Painting channel information. To do so, we replaced CLOOME's ResNet image encoder with our CrossChannelFormer encoder. To isolate the effects of CrossChannelFormer's ability to reason across channels from the effects of per-perturbation pooling, in this experiment we removed the CrossChannelFormer's pooling function and trained the resulting model (denoted as CrossChannelFormer\(^{\dagger}\) in Table above) on individual image-perturbation pairs.
            We found that this change led to an additional increase in model performance. To understand how our approach compared to previous channel-aware vision encoding approaches, we ran this same experiment using a channel agnostic MAE (CA-MAE) and ViT (ChannelViT) as vision encoders. We found that CrossChannelFormer consistently outperformed these baselines on both tasks. This demonstrates that image embeddings extracted from models pretrained on natural images can be effectively leveraged with CrossChannelFormer. Additionally, our approach substantially reduced training time, achieving a 3.9 times speedup compared to CLOOME and a 2.2 times speedup compared to other channel-agnostic methods. This efficiency stems from CrossChannelFormer operating in a compact feature space and requiring only \(C+1\) tokens per instance. 
          </p>
          <div class="content has-text-centered" style="margin-top: 2rem;">
            <img src="static/images/result4.jpg" style="width: 100%; height: auto;" />
            <p class="is-size-10" style="margin-top: 1rem;">
              <em>
              Retrieval performance of CellCLIP trained across different pooling strategies on perturb-to-profile and profile-to-perturb tasks.
              </em>
            </p>
          </div>
          <h4>
          Pooling yields improved alignment and computational efficiency
          </h4>          
          <p>
          Finally, we evaluated the impact of the attention-based pooling operator within CrossChannelFormer compared to instance-level training (i.e., no pooling). We found that including pooling resulted in yet another increase in model performance. In addition, by reducing the number of pairs for contrastive loss computation, pooling yielding a 6.7 times speedup in training time relative to instance-level training. We also explored the impact of varying our choice of pooling operator, and found that attention-based pooling yielded the best results among the pooling operators we considered.
          </p>
          <p>
          Overall, our results demonstrate that each of our main design choices in CellCLIP contributes to improved retrieval performance and computational efficiency compared to prior work.
          </p>
          <h4>Recovering known biological relationships</h4>
          <p>
           We report recall of known biological relationships among genetic perturbations in RxRx3-core. Across all benchmark databases, we found that CellCLIP achieved the best recovery of known gene-gene relationships compared to baseline models. Notably, we found that replacing the CWCL loss used in CellCLIP with the standard CLIP loss leads to worse performance on this task, illustrating the benefits of using soft labeling for alignment in the image profile space \( \mathcal{P} \). Altogether, these results further demonstrate that CellCLIP can recover meaningful relationships between perturbations in its image profile latent space.
          </p>
          <div class="content has-text-centered" style="margin-top: 2rem;">
            <img src="static/images/result2.jpg" style="width: 100%; height: auto;" />
            <p class="is-size-10" style="margin-top: 1rem;">
              <em>
              Zero-shot gene–gene relationship recovery on RxRx3-core, evaluated across varying thresholds from Recall@2% [0.01,0.99] (top and bottom 1%) to Recall@20 % [0.10,0.90] (top and bottom 10%), using pathway annotations from CORUM, HuMAP, Reactome, SIGNOR, and STRING.
              </em>
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
    


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{lu2025cellclip,
  title={CellCLIP--Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning},
  author={Lu, Mingyu and Weinberger, Ethan and Kim, Chanwoo and Lee, Su-In},
  journal={arXiv preprint arXiv:2506.06290},
  year={2025}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
